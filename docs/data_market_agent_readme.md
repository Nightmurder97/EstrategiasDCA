# Data Market Agent - README

This document provides an overview of the Data Market Agent, its structure, and how to use its components. This agent is designed to collect, process, and provide market data from various external sources to enhance the existing DCA trading system.

## 1. Agent Structure

The Data Market Agent resides within the `src/data_market_agent/` directory. Key components include:

*   **`__init__.py`**: Makes the agent's components available as a package.
*   **`base_collector.py`**: Defines an abstract base class `BaseCollector` which all specific data collectors should inherit from. It provides a common interface for fetching and processing data.
*   **`agent_scheduler.py`**: Contains the `AgentScheduler` class, which uses `APScheduler` to schedule and run data collection tasks periodically.
*   **`models.py`**: Defines Pydantic models (e.g., `CollectedDataItem`, `DefiLlamaProtocol`, `DefiLlamaYield`) used to structure the data collected by the agent.
*   **`defillama_collector.py`**: Implements the `DefiLlamaCollector` for fetching data from DeFiLlama APIs (e.g., protocols, TVL, yield opportunities).

Associated tests are located in `tests/test_defillama_collector.py`.

## 2. Running and Testing the DeFiLlama Collector

### Dependencies
Ensure all dependencies are installed from the main `requirements.txt` file:
```bash
pip install -r requirements.txt
```
This includes `aiohttp` (for async HTTP requests by `DefiLlamaCollector`) and `APScheduler`.

### Direct Testing of `DefiLlamaCollector`
The `src/data_market_agent/defillama_collector.py` script contains a `if __name__ == "__main__":` block that allows for direct execution to test its data fetching capabilities.

To run this test:
```bash
python src/data_market_agent/defillama_collector.py
```
This will:
1.  Initialize the `DefiLlamaCollector`.
2.  Attempt to fetch all protocols from DeFiLlama and print the first few.
3.  Attempt to fetch TVL for a specific protocol (e.g., "uniswap") and print some details.
4.  Attempt to fetch all yield opportunities and print details for the first few.

Output will be printed to the console, including logs if basic logging is configured.

### Running Unit Tests
Unit tests for the `DefiLlamaCollector` are located in `tests/test_defillama_collector.py`. These tests use `pytest` and mock external API calls.

To run these tests (from the project root directory):
```bash
pytest tests/test_defillama_collector.py
```
Ensure `pytest` and `pytest-asyncio` (if not already a direct dependency, it's often needed for async tests with pytest) are available in your environment. `pytest` is listed in the main `requirements.txt`.

## 3. Integration with Existing DCA System (Initial Plan)

The primary method for integrating the Data Market Agent's insights (starting with DeFiLlama) into the existing DCA system is via a scheduled, file-based data exchange:

1.  **Scheduled Collection**:
    *   An instance of `AgentScheduler` will be configured to run the `DefiLlamaCollector.fetch_and_process()` method (or a wrapper function) at regular intervals (e.g., every 2 hours).
    *   This wrapper function will take the `CollectedDataItem` objects returned by the collector.
    *   It will serialize these items (e.g., convert Pydantic models to dictionaries) and save them as JSON files.
    *   Output directory: `data/market_insights/defillama/defillama_data_[timestamp].json`

2.  **Consumption by DCA System**:
    *   The existing `src/market_analysis.py` module (or a new, dedicated module within the DCA system) will be enhanced to:
        *   Check the `data/market_insights/defillama/` directory for the latest data file.
        *   Load and parse the JSON data.
        *   Extract relevant insights (e.g., top APY opportunities, significant TVL changes).
    *   These insights can then be:
        *   Included in reports generated by `src/report_generator.py`.
        *   Potentially used by `src/dca_live_trader.py` or `src/portfolio_optimizer.py` for decision-making, although this would be a more advanced integration step.

## 4. Next Steps (Preparing for Week 2 and Beyond)

This initial framework and DeFiLlama integration lay the groundwork for further enhancements:

*   **BingX AI Scraper & Premium APIs (Week 2-3)**:
    *   New collectors (e.g., `BingXCollector`, `MessariCollector`) will be created in `src/data_market_agent/`, inheriting from `BaseCollector`.
    *   These collectors will implement the logic for fetching data from their respective sources (web scraping for BingX AI, API calls for Messari, CoinGlass).
    *   The `AgentScheduler` will be updated to include jobs for these new collectors.
    *   Data from these sources will also be saved to `data/market_insights/` (e.g., in subdirectories like `bingx/`, `messari/`) or processed into a more unified data store if needed.

*   **Advanced Analytics Engine**:
    *   The data collected by various agents can be fed into a more sophisticated analytics engine. This might involve:
        *   Storing data in a time-series database (like InfluxDB, as mentioned in the full project spec) for easier querying and analysis.
        *   Developing modules for correlation analysis, sentiment trend analysis, etc.

*   **AI Enhancements (Transformers, GNNs, RL - Phase 3)**:
    *   The structured data collected by the Data Market Agent will serve as crucial input for these advanced AI models.
    *   For example, news and social media sentiment data (planned for later collectors) can be processed by Transformer models.
    *   Market data and DeFi protocol relationships can be used to build Graph Neural Networks.

By creating a modular `data_market_agent`, we can incrementally add new data sources and enhance the intelligence available to the core DCA trading system.
The current file-based integration is a starting point; this could evolve into a more robust inter-process communication (IPC) mechanism or a dedicated API for the Data Market Agent if the system complexity grows.
---
This README provides a basic guide. Further details will be added as the agent evolves.
